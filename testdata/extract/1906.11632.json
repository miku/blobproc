{"sha1hex":"a93ed137ab87f59cf03a491d097ddd396fa7ebca","status":"success","fileinfo":{"size":1215637,"sha1hex":"a93ed137ab87f59cf03a491d097ddd396fa7ebca","sha256hex":"5da99da592ca16f51dc204ff1ad54070487e231eab57bf120dbf61a6e9a80767","md5hex":"3b5e8d09f374d104f133e6e199e1b16b","mimetype":"application/pdf"},"text":"                                                                       A Survey on GANs for Anomaly Detection\n\n\n                                                          Federico Di Mattia 1 * Paolo Galeone 1 * Michele De Simoni 1 * Emanuele Ghelfi 1 *\n\n\n                                                                  Abstract                                  mulation, adding the learning of the inverse mapping which\n                                                                                                            maps the data back to the latent representation. A learned\n                                              Anomaly detection is a significant problem faced\n                                                                                                            function that maps input data to its latent representation to-\n                                              in several research areas. Detecting and correctly\n                                                                                                            gether with a function that does the opposite (the generator)\narXiv:1906.11632v1 [cs.LG] 27 Jun 2019\n\n\n\n\n                                              classifying something unseen as anomalous is\n                                                                                                            is the basis of the anomaly detection using GANs.\n                                              a challenging problem that has been tackled in\n                                              many different manners over the years. Gener-                 The paper is organized as follows. In Section 1 we introduce\n                                              ative Adversarial Networks (GANs) and the ad-                 the GANs framework and, briefly, its most innovative exten-\n                                              versarial training process have been recently em-             sions, namely conditional GANs and BiGAN, respectively\n                                              ployed to face this task yielding remarkable re-              in Section 1.2 and Section 1.3. Section 2 contains the state\n                                              sults. In this paper we survey the principal GAN-             of the art architectures for anomaly detection with GANs.\n                                              based anomaly detection methods, highlighting                 In Section 3 we empirically evaluate all the analyzed archi-\n                                              their pros and cons. Our contributions are the                tectures. Finally, Section 4 contains the conclusions and\n                                              empirical validation of the main GAN models                   future research directions.\n                                              for anomaly detection, the increase of the experi-\n                                              mental results on different datasets and the public           1.1. GANs\n                                              release of a complete Open Source toolbox for\n                                              Anomaly Detection using GANs.                                 GANs are a framework for the estimation of generative\n                                                                                                            models via an adversarial process in which two models, a\n                                                                                                            discriminator D and a generator G, are trained simultane-\n                                                                                                            ously. The generator G aim is to capture the data distribu-\n                                         1. Introduction                                                    tion, while the discriminator D estimates the probability that\n                                         Anomalies are patterns in data that do not conform to a            a sample came from the training data rather than G. To learn\n                                         well-defined notion of normal behavior (Chandola et al.,           a generative distribution pg over the data x the generator\n                                         2009). Generative Adversarial Networks (GANs) and the              builds a mapping from a prior noise distribution pz to a data\n                                         adversarial training framework (Goodfellow et al., 2014)           space as G(z; θG ), where θG are the generator parameters.\n                                         have been successfully applied to model complex and high           The discriminator outputs a single scalar representing the\n                                         dimensional distribution of real-world data. This GAN char-        probability that x came from real data rather than from pg .\n                                         acteristic suggests they can be used successfully for anomaly      The generator function is denoted with D(x; θD ), where θD\n                                         detection, although their application has been only recently       are discriminator parameters.\n                                         explored. Anomaly detection using GANs is the task of              The original GAN framework (Goodfellow et al., 2014)\n                                         modeling the normal behavior using the adversarial training        poses this problem as a min-max game in which the two\n                                         process and detecting the anomalies measuring an anomaly           players (G and D) compete against each other, playing the\n                                         score (Schlegl et al., 2017). To the best of our knowledge,        following zero-sum min-max game:\n                                         all the GAN-based approaches to anomaly detection build\n                                         upon on the Adversarial Feature Learning idea (Donahue                 min max V (D, G) = Ex∼pdata (x) [log D(x)]+\n                                                                                                                  G    D\n                                         et al., 2016) in which the BiGAN architecture has been                                                                       (1)\n                                                                                                                           Ez∼pz (z) [log (1 − D(G(z)))] .\n                                         proposed. In their original formulation, the GAN frame-\n                                         work learns a generator that maps samples from an arbitrary\n                                         latent distribution (noise prior) to data as well as a discrimi-\n                                                                                                            1.2. Conditional GANs\n                                         nator which tries to distinguish between real and generated\n                                         samples. The BiGAN architecture extended the original for-         GANs can be extended to a conditional model (Mirza \u0026\n                                           *                                                                Osindero, 2014) conditioning either G or D on some ex-\n                                             Equal contribution 1 Zuru Tech, Modena, Italy. Correspon-\n                                         dence to: Federico Di Mattia \u003cfederico.d@zuru.tech\u003e.               tra information y. The y condition could be any auxiliary\n                                                                                                            information, such as class labels or data from other modal-\n                                                                                                            ities. We can perform the conditioning by feeding y into\n\f                                             GANs for Anomaly Detection: a survey\n\n\n\n\n                                Figure 1. The structure of BiGAN proposed in (Donahue et al., 2016).\n\n\nboth the discriminator and generator as an additional input         performance issues of AnoGAN a BiGAN-based approach\nlayer. The generator combines the noise prior pz (z) and            has been proposed in Zenati et al. (2018), here referred as\ny in a joint hidden representation, the adversarial training        EGBAD (Efficient GAN Based Anomaly Detection), that\nframework allows for considerable flexibility in how this           outperformed AnoGAN execution time. Recently, Akcay\nhidden representation is composed. In the discriminator, x          et al. (2018) advanced a GAN + autoencoder based approach\nand y are presented as inputs to a discriminative function.         that exceeded EGBAD performance from both evaluation\nThe objective function considering the condition becomes:           metrics and execution speed.\n                                                                    In the following sections, we present an analysis of the\n                                                                    considered architecture. The term sample and image are\n    min max V (D, G) = Ex∼pdata (x|y) [log D(x)]+\n     G    D\n                                                             (2)    used interchangeably since GANs can be used to detect\n               Ez∼pz (z) [log(1 − D(G(z|y)))].                      anomalies on a wide range of domains, but all the analyzed\n                                                                    architectures focused mostly on images.\n1.3. BiGAN\n                                                                    2.1. AnoGAN\nBidirectional GAN (Donahue et al., 2016) extends the GAN\nframework including an encoder E(x; θE ) that learns the            AnoGAN aim is to use a standard GAN, trained only on\ninverse of the generator E = G−1 . The BiGAN training               positive samples, to learn a mapping from the latent space\nprocess allows learning a mapping simultaneously from               representation z to the realistic sample x̂ = G(z) and use\nlatent space to data and vice versa. The encoder E is a             this learned representation to map new, unseen, samples\nnon-linear parametric function in the same way as G and             back to the latent space. Training a GAN on normal samples\nD, and it can be trained using gradient descent. As in the          only, makes the generator learn the manifold X of normal\nconditional GANs scenario, the discriminator must learn to          samples. Given that the generator learns how to generate\nclassify not only real and fake samples, but pairs in the form      normal samples, when an anomalous image is encoded its\n(G(z), z) or (x, E(x)). The BiGAN training objective is:            reconstruction will be non-anomalous; hence the difference\n                                                                    between the input and the reconstructed image will highlight\n                                                                    the anomalies. The two steps of training and detecting\n  min max V (D, G, E) =                                             anomalies are summarized in Figure 2.\n   G,E   D\n                                                             (3)    The authors have defined the mapping of input samples to\n              Ex∼pdata (x) [Ez∼pE(z|x) [log D(x, z)]]+\n                                                                    the latent space as an iterative process. The aim is to find a\n              Ez∼pz (z) [Ex∼pG (x|z) [log(1 − D(x, z)))]].          point z in the latent space that corresponds to a generated\n                                                                    value G(z) that is similar to the query value x located on\nFigure 1 depicts a visual structure of the BiGAN architec-          the manifold X of the positive samples. The research pro-\nture.                                                               cess is defined as the minimization trough γ = 1, 2, . . . , Γ\n                                                                    backpropagation steps of the loss function defined as the\n                                                                    weighted sum of the residual loss LR and discriminator loss\n2. GANs for anomaly detection\n                                                                    LD , in the spirit of Yeh et al. (2016).\nAnomaly detection using GANs is an emerging research\n                                                                    The residual loss measures the dissimilarity between the\nfield. Schlegl et al. (2017), here referred to as AnoGAN,\n                                                                    query sample and the generated sample in the input domain\nwere the first to propose such a concept. In order to face the\n\f                                                GANs for Anomaly Detection: a survey\n\n\n\n\nFigure 2. AnoGAN (Schlegl et al., 2017). The GAN is trained on positive samples. At test time, after Γ research iteration the latent vector\nthat maps the test image to its latent representation is found zΓ . The reconstructed image G(zΓ ) is used to localize the anomalous regions.\n\n\nspace:                                                                     • Used the same mapping scheme to define an anomaly\n                                                                             score.\n                  LR (zγ ) = ||x − G(zγ )||1 .                  (4)\n                                                                        Cons\nThe discriminator loss takes into account the discriminator\nresponse. It can be formalized in two different ways. Follow-\ning the original idea of Yeh et al. (2016), hence feeding the              • Requires Γ optimization steps for every new input: bad\ngenerated image G(zγ ) into the discriminator and calculat-                  test-time performance.\ning the sigmoid cross-entropy as in the adversarial training\n                                                                           • The GAN objective has not been modified to take into\nphase: this takes into account the discriminator confidence\n                                                                             account the need for the inverse mapping learning.\nthat the input sample is derived by the real data distribution.\nAlternatively, using the idea introduced by Salimans et al.                • The anomaly score is difficult to interpret, not being in\n(2016), and used by the AnoGAN (Schlegl et al., 2017)                        the probability range.\nauthors, to compute the feature matching loss, extracting\nfeatures from a discriminator layer f in order to take into             2.2. EGBAD\naccount if the generated sample has similar features of the\ninput one, by computing:                                                Efficient GAN-Based Anomaly Detection (EGBAD) (Zenati\n                                                                        et al., 2018) brings the BiGAN architecture to the anomaly\n              LD (zγ ) = ||f (x) − f (G(zγ ))||1 ,              (5)     detection domain. In particular, EGBAD tries to solve the\n                                                                        AnoGAN disadvantages using Donahue et al. (2016) and Du-\nhence the proposed loss function is:                                    moulin et al. (2017) works that allows learning an encoder\n                                                                        E able to map input samples to their latent representation\n                                                                        during the adversarial training. The importance of learning\n         L(zγ ) = (1 − λ) · LR (zγ ) + γ · LD (zγ ).            (6)\n                                                                        E jointly with G is strongly emphasized, hence Zenati et al.\n                                                                        (2018) adopted a strategy similar to the one indicated in\nIts value at the Γ-th step coincides with the anomaly score             Donahue et al. (2016) and Dumoulin et al. (2017) in order\nformulation:                                                            to try to solve, during training, the optimization problem\n                                                                        minG,E maxD V (D, E, G) where V (D, E, G) is defined\n                        A(x) = L(zΓ ).                          (7)\n                                                                        as in Equation 3. The main contribution of the EGBAD is\n                                                                        to allow computing the anomaly score without Γ optimiza-\nA(x) has no upper bound; to high values correspond an                   tion steps during the inference as it happens in AnoGAN\nhigh probability of x to be anomalous.                                  (Schlegl et al., 2017).\nIt should be noted that the minimization process is required\nfor every single input sample x.                                        2.3. GANomaly\n                                                                        Akcay et al. (2018) introduce the GANomaly approach. In-\n2.1.1. P ROS AND CONS                                                   spired by AnoGAN (Schlegl et al., 2017), BiGAN (Donahue\nPros                                                                    et al., 2016) and EGBAD (Zenati et al., 2018) they train a\n                                                                        generator network on normal samples to learn their mani-\n   • Showed that GANs can be used for anomaly detection.                fold X while at the same time an autoencoder is trained to\n                                                                        learn how to encode the images in their latent representa-\n   • Introduced a new mapping scheme from latent space                  tion efficiently. Their work is intended to improve the ideas\n     to input data space.                                               of Schlegl et al. (2017), Donahue et al. (2016) and Zenati\n\f                                               GANs for Anomaly Detection: a survey\n\n\n\n\n                                                                                             Real / Fake\n\n\n\n\n              Input/Output        Conv      LeakyReLU      BatchNorm       ConvTranspose     ReLU          Tanh   Softmax\n\n\n                             Figure 3. GANomaly architecture and loss functions from (Akcay et al., 2018).\n\n\net al. (2018). Their approach only needs a generator and a             to Figure 3 for a visual representation of the architecture\ndiscriminator as in a standard GAN architecture.                       underpinning GANomaly.\nGenerator network The generator network consists of                    The GANomaly architecture differs from AnoGAN (Schlegl\nthree elements in series, an encoder GE a decoder GD                   et al., 2017) and from EGBAD (Zenati et al., 2018). In\n(both assembling an autoencoder structure) and another en-             Figure 4 the three architectures are presented.\ncoder E. The architecture of the two encoders is the same.\n                                                                       Beside these two networks, the other main contribution of\nGE takes in input an image x and outputs an encoded ver-\n                                                                       GANomaly is the introduction of the generator loss as the\nsion z of it. Hence, z is the input of GD that outputs x̂,\n                                                                       sum of three different losses; the discriminator loss is the\nthe reconstructed version of x. Finally, x̂ is given as an\n                                                                       classical discriminator GAN loss.\ninput to the encoder E that produces ẑ. There are two main\ncontributions from this architecture. First, the operating             Generator loss The objective function is formulated by\nprinciple of the anomaly detection of this work lies in the            combining three loss functions, each of which optimizes a\nautoencoder structure. Given that we learn to encode nor-              different part of the whole architecture.\nmal (non-anomalous) data (producing z) and given that we\n                                                                       Adversarial Loss The adversarial loss it is chosen to be the\nlearn to generate normal data (x̂) starting from the encoded\n                                                                       feature matching loss as introduced in Schlegl et al. (2017)\nrepresentation z, when the input data x is an anomaly its\n                                                                       and pursued in Zenati et al. (2018):\nreconstruction will be normal. Because the generator will al-\nways produce a non-anomalous image, the visual difference\nbetween the input x and the produced x̂ will be high and in                   Ladv = Ex∼pX ||f (x) − Ex∼pX f (G(x))||2 ,        (8)\nparticular will spatially highlight where the anomalies are\nlocated. Second, the encoder E at the end of the generator             where f is a layer of the discriminator D, used to extract\nstructure helps, during the training phase, to learn to encode         a feature representation of the input . Alternatively, binary\nthe images in order to have the best possible representation           cross entropy loss can be used.\nof x that could lead to its reconstruction x̂.\n                                                                       Contextual Loss Through the use of this loss the genera-\nDiscriminator network The discriminator network D is                   tor learns contextual information about the input data. As\nthe other part of the whole architecture, and it is, with the          shown in (Isola et al., 2016) the use of the L1 norm helps to\ngenerator part, the other building block of the standard GAN           obtain better visual results:\narchitecture. The discriminator, in the standard adversarial\ntraining, is trained to discern between real and generated\n                                                                                      Lcon = Ex∼pX ||x − G(x)||1 .              (9)\ndata. When it is not able to discern among them, it means\nthat the generator produces realistic images. The generator\n                                                                       Encoder Loss This loss is used to let the generator network\nis continuously updated to fool the discriminator. Refer\n                                                                       learn how to best encode a normal (non-anomalous) image:\n\f                                               GANs for Anomaly Detection: a survey\n\n\n\n\n   z     G(z)     x'                       z    G(z)       x'                      x GE(x) z    GD(z)    x' E(x') z'\n\n\n\n\n                   x       D(x, x')       z'   E(x)    x           D(x, x')                                 D(x, x')    real/fake\n    A                                 B                                        C\n\n\nFigure 4. Architectures comparison. A) AnoGAN (Schlegl et al., 2017), B) EGBAD (Zenati et al., 2018), C) GANomaly (Akcay et al.,\n2018).\n\n\n                                                                          • The contextual loss can be used to localize the anomaly.\n           Lenc = Ex∼pX ||GE (x) − E(G(x))||2 .                 (10)\n                                                                        Cons\nThe resulting generator loss will be the result of the\n(weighted) sum of the three previously defined losses:\n                                                                          • It allows to detect anomalies both in the image space\n                                                                            and in the latent space, but the results couldn’t match:\n        L = wadv Ladv + wcon Lcon + wenc Lenc ,                 (11)        a higher anomaly score, that’s computed only in the\n                                                                            latent space, can be associated with a generated sample\nwhere wadv , wcon and wenc are weighting parameters used                    with a low contextual loss value and thus very similar\nto adjust the importance of the three losses.                               to the input - and vice versa.\nAt test stage, the authors proposed to compute the anomaly\nscore in a different way respect to AnoGAN: only using                    • Defines a new anomaly score.\nusing Lenc :\n                                                                        3. Experiments\n                A(x) = ||GE (x) − E(G(x))||2 .                  (12)\n                                                                        To evaluate the performance of every Anomaly Detection\nIn order to make the anomaly score easier to interpret, they            algorithm described we re-implemented all of them using\nproposed to compute the anomaly score for every sample x̂               the popular deep learning framework Tensorflow (Abadi\nin thentest set D̂, getting a o\n                              set of individual anomaly scores:         et al., 2015) creating a publicly available toolbox for the\n                                                                        Anomaly Detection with GANs.\nS = si : A(x̂i ) , x̂i ∈ D̂ and then apply feature scaling\nto have the anomaly scores within the probabilistic range of            Experiments were made trying to improve, where possible\n[0, 1]:                                                                 (i.e., where there were known errors communicated directly\n                                                                        by the authors), the code. The results shown in the following\n                                                                        sections are the best empirically obtained among all the\n                             si + min(S)\n                   s0i =                   .                    (13)    tests carried out and do not necessarily derive from the\n                           max(S) − min(S)                              configurations of the networks that at a theoretical level\n                                                                        should be the correct ones. In particular, as described in\n2.3.1. P ROS AND CONS\n                                                                        Appendix A, in version 1 (Appendix A.1) of our tests, with\nPros                                                                    the correct implementation of the BiGAN/EGBAD models,\n                                                                        we obtained worse results than those obtained in version 2\n  • An encoder is learned during the training process,                  (Appendix A.2) of our tests with the architecture already\n    hence we don’t have the need for a research process as              implemented in the reference paper, although this contained\n    in AnoGAN (Schlegl et al., 2017).                                   known errors.\n\n  • Using an autoencoder like architecture (no use of noise             3.1. Experimental setup\n    prior) makes the entire learning process faster.                                                                           TM\n                                                                        The experiments are performed using a Intel R Core          i7-\n  • The anomaly score is easier to interpret.                           7820X CPU and NVIDIA R GTX 1080 Ti.\n\f                                                     GANs for Anomaly Detection: a survey\n\n          BiGAN/EGBAD and GANomaly performance comparison on MNIST and Fashion-MNIST datasets\n\n                    BiGAN/EGBAD                                                           BiGAN/EGBAD\n           1         GANomaly\n                                                                                 1         GANomaly\n\n\n\n\n          0.8                                                                   0.8\n  AUPRC\n\n\n\n\n                                                                        AUPRC\n          0.6                                                                   0.6\n\n\n          0.4                                                                   0.4\n\n\n          0.2                                                                   0.2\n\n\n                0       1         2    3    4    5    6    7    8   9                 0       1         2   3   4   5   6   7   8    9\n                                  (a) MNIST anomalous classes                                 (b) Fashion-MNIST anomalous classes\n\nFigure 5. Performance on MNIST (a) and Fashion-MNIST (b) of BiGAN/EGBAD and GANomaly models measured by the area under\nthe precision-recall curve (AUPRC). The results have been selected as the best result between different type of training (with bce/fm and\nwith residual loss). Best viewed in color.\n\n\n3.1.1. DATASETS                                                           CIFAR-10 consists of 60000 32 × 32 color images in 10\n                                                                          classes, with 6000 images per class. There are 50000 train-\nWe decided to train and test on widely known datasets com-\n                                                                          ing images and 10000 test images.\nmonly found in the literature: MNIST (LeCun \u0026 Cortes,\n2010a), FashionMNIST (Xiao et al., 2017), CIFAR-10                        KDD: KDD dataset (Lichman, 1999) consists in a collec-\n(Krizhevsky et al., 2007), and KDD99 (Lichman, 1999).                     tion of network intrusion detection data, this is the data set\n                                                                          used for The Third International Knowledge Discovery and\nMNIST: MNIST dataset (LeCun \u0026 Cortes, 2010b) is a\n                                                                          Data Mining Tools Competition held in conjunction with\ndatabase of handwritten digits. It consists of 28 × 28 pixels\n                                                                          KDD-99 The Fifth International Conference on Knowledge\ngrayscale images split in a training set of 60,000 examples\n                                                                          Discovery and Data Mining. The competition task was to\nand a test set of 10,000 examples. The output classes are 10\n                                                                          build a network intrusion detector, a predictive model ca-\nin total and represent the ten digits from 0 to 9. In order to\n                                                                          pable of distinguishing between “bad” connections, called\nreplicate the results of the papers presented in this work, the\n                                                                          intrusions or attacks, and “good” normal connections. All\ntraining process and the subsequent testing phase have been\n                                                                          the examples are string tuples or numbers. Given the con-\nevaluated on each class, i.e., one at a time, only one class\n                                                                          siderable amount of data inside the data set, we used as in\nhas been considered as the anomaly class, and the remaining\n                                                                          (Zenati et al., 2018), the KDDCUP99 10 percent version of\nnine classes have been considered together as the normal\n                                                                          the data set. Since the anomalies are in a higher percentage\ndata.\n                                                                          than normal data, the normal ones have been considered as\nFashion MNIST: Fashion-MNIST dataset (Xiao et al.,                        anomalies.\n2017) is a dataset intended to be a more complex drop-\nin replacement for the traditional MNIST dataset, developed               3.1.2. M ETHODOLOGY\nby Zalando R Research Group. It shares with the MNIST\n                                                                          We follow the same methodology employed by Zenati et al.\ndataset the same image size and train-test split; it consists\n                                                                          in the official code accompanying Zenati et al. (2018).\nof 28 × 28 pixels images split in a training set of 60,000\nexamples and a test set of 10,000 examples. Each image is                 We start by getting all the dataset together (train and test\na grayscale image associated, as the MNIST dataset, with a                split). Starting from this one big pool of examples, we\nlabel from 10 classes, from 0 to 9. Each example represents               choose one class as an anomaly and, after shuffling the\na different item of clothing. As for the MNIST dataset, the               dataset, we then create a training set by using 80% of the\ntraining and the following test phase were conducted on                   whole data while the remaining 20% is used for the testing\neach class, one at a time.                                                set, this process is repeated for all the classes in the dataset.\n                                                                          Each model is trained accordingly to its original implemen-\nCIFAR-10: A dataset of tiny images of various subjects,\n                                                                          tation on the training set and is tested on the whole dataset\n\f                                              GANs for Anomaly Detection: a survey\n\n                         Train                    Test                                     Train                Test\n               BCE     FM        Residual      BCE       FM                             BCE     FM        BCE          FM\n    Case 1      X                   X           X        X                    Case 1     X                  X          X\n    Case 2              X           X           X        X                    Case 2               X        X          X\n    Case 3      X                               X        X\n    Case 4              X                       X        X          Table 2. GANomaly - Training and testing configuration combina-\n                                                                    tions.\nTable 1. BiGAN - Training and testing configuration combinations.\n\n                                                                                                          KDD\nmade up of both regular and anomalous data.                                                Precision     Recall         F1-Score\n                                                                     BiGAN/EGBAD           0.941174     0.956155        0.948605\n3.2. Results                                                         GANomaly              0.830256     0.841112        0.835648\nAll tests have been made measuring the area under precision\nand recall curve (AUPRC). For a complete understanding of           Table 3. The performances of BiGAN/EGBAD and GANomaly\n                                                                    models on the KDD dataset.\nall the results, please refer to Appendix A. Our fully static-\nTensorflow implementation (i.e., only Tensorflow frame-\nwork has been used, we relied upon neither Keras nor eager\nexecution) produced different results from the one depicted         FM influences only the generator loss. The residual loss that\nin the original Donahue et al. (2016), Schlegl et al. (2017),       it is intended as the difference between the original image\nand Akcay et al. (2018) papers. The tests have been made by         and the image reconstructed by the generator starting from\ntraining the GAN networks with different hyper-parameters           a latent representation (please refer to Equation (4)) when\nconfigurations in order to test a broader range of models           used has been added as an additional term to the BCE loss\nconfigurations. Moreover, following the GANomaly ap-                of the encoder. In Figure 5 is possible to see the results on\nproach, we have added an evaluation process to make model           the MNIST and Fashion-MNIST datasets. In Appendix A\nselection and thus select the best model during the training        it is possible to see the complete results. The images used\nphase. The model selection has been made for BiGAN and              are the original 28 × 28 images. Any additional result from\nGANomaly, since doing it for AnoGAN would be unfeasi-               multiple different tests performed with different random\nble, due to the Γ research steps required for every sample          seeds has been omitted for the sake of clarity.\nat test-time. We intentionally left out the performance eval-\n                                                                    GANomaly: GANomaly training and testing have been\nuation of the AnoGAN model. Due to the inner workings\n                                                                    done similarly to the previously described BiGAN/EGBAD\nof the architecture that should have required a very long\n                                                                    architecture. The whole procedure is leaner here, with only\ntime to be tested because of the necessity to find, every time\n                                                                    the combinations shown in Table 2.\nand for every image taken into consideration, the best latent\nrepresentation, meaning that for every image we would need          We added a step of model selection during the training phase\n500 training step (500 it is an empirical value found in the        in order to always save the very best model. For this archi-\noriginal paper (Schlegl et al., 2017)). Following, the training     tecture, the testing phase has been done using an anomaly\nand test methods are described. A summary of the results            score equal to the squared difference between the latent\nis present in Figure 5 for the MNIST and Fashion-MNIST              representations of the image encoded first with autoencoder\ndataset, and in Table 3 for the KDD results.                        part of the network and, after being reconstructed, encoded\n                                                                    again with the encoder. To briefly review the working of\nBiGAN/EGBAD: With BiGAN/EGBAD (Zenati et al.,\n                                                                    GANomaly, see Figure 3. The images used are the original\n2018) architecture we have executed the highest number\n                                                                    ones resized to 32 × 32. Refer to the plot on the right of\nof training and testing configurations, this means we have\n                                                                    Figure 5 to a brief summary of the results on MNIST and\nperformed, for every label the combinations depicted in Ta-\n                                                                    Fashion-MNIST datasets and on Table 3 for the results on\nble 1. To better understand what the table shows, taking\n                                                                    KDD dataset.\nthe first row (Case 1) as an example, we are describing the\ncase where, during the training phase, a Binary Cross En-           AnoGAN: As previously stated, the results of AnoGAN\ntropy loss (BCE) in combination with a Residual loss has            have not been reproduced due to the onerous computational\nbeen used and, during the testing phase, we computed the            requirements causing the train and test phases to be in-\nanomaly score twice, using the feature matching (FM) and            tensively time-consuming. The reader could refer to the\nthe BCE. All the other cases should be clear. In particular,        original paper (Schlegl et al., 2017) to an overview of the\nduring the training phase, the choice between using BCE or          results.\n\f                                            GANs for Anomaly Detection: a survey\n\n4. Conclusions                                                   Krizhevsky, A., Nair, V., and Hinton, G. CIFAR-10 (Cana-\n                                                                   dian Institute for Advanced Research). 2007. URL http:\nWe implemented and evaluated the state-of-the-art algo-            //www.cs.toronto.edu/˜kriz/cifar.html.\nrithms for anomaly detection based on GANs. In this work,\nwe unified the three major works under the same framework        LeCun, Y. and Cortes, C. MNIST handwritten digit database.\n(Tensorflow) and within the same code-base. The analysis           2010a. URL http://yann.lecun.com/exdb/\nand implementation of the algorithms allowed us to ver-            mnist/.\nify the effectiveness of the GANs-based approach to the\nanomaly detection problem and, at the same time, highlight       LeCun, Y. and Cortes, C. MNIST handwritten digit database.\nthe differences between the original papers and the publicly       2010b. URL http://yann.lecun.com/exdb/\navailable code. Besides, to test the feasibility of the archi-     mnist/.\ntectures mentioned above, this work intends to provide a         Lichman, M. KDD Cup 1999 Data (University of California\nmodular and ready-to-go solution for everyone desiring an          Irvine). 1999. URL http://kdd.ics.uci.edu/\nanomaly detection toolkit working out of the box.                  databases/kddcup99/kddcup99.html.\n\nReferences                                                       Mirza, M. and Osindero, S. Conditional Generative Ad-\n                                                                  versarial Nets. abs/1411.1784, 2014. URL http:\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,         //arxiv.org/abs/1411.1784.\n  Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\n  Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,     Salimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V.,\n  M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-        Radford, A., and Chen, X. Improved Techniques for\n  enberg, J., Man, D., Monga, R., Moore, S., Murray, D.,           Training GANs. abs/1606.03498, 2016. URL http:\n  Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,      //arxiv.org/abs/1606.03498.\n  I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\n                                                                 Schlegl, T., Seebck, P., Waldstein, S. M., Schmidt-Erfurth,\n Vigas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,\n                                                                   U., and Langs, G. Unsupervised Anomaly Detection with\n  M., Yu, Y., and Zheng, X. TensorFlow: Large-Scale Ma-\n                                                                   Generative Adversarial Networks to Guide Marker Dis-\n  chine Learning on Heterogeneous Systems. 2015. URL\n                                                                   covery. abs/1703.05921, 2017. URL http://arxiv.\n  https://www.tensorflow.org/. Software avail-\n                                                                   org/abs/1703.05921.\n  able from tensorflow.org.\n                                                                 Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST:\nAkcay, S., Abarghouei, A. A., and Breckon, T. P.\n                                                                   A Novel Image Dataset for Benchmarking Machine\n  GANomaly: Semi-Supervised Anomaly Detection via\n                                                                   Learning Algorithms. abs/1708.07747, 2017. URL\n Adversarial Training. abs/1805.06725, 2018. URL\n                                                                   http://dblp.uni-trier.de/db/journals/\n  http://arxiv.org/abs/1805.06725.\n                                                                   corr/corr1708.html#abs-1708-07747.\nChandola, V., Banerjee, A., and Kumar., V. Anomaly detec-\n                                                                 Yeh, R. A., Chen, C., Lim, T.-Y., Hasegawa-Johnson, M.,\n  tion: A survey. 41(3), 2009.\n                                                                   and Do, M. N. Semantic Image Inpainting with Percep-\nDonahue, J., Krhenbhl, P., and Darrell, T. Adversarial             tual and Contextual Losses. abs/1607.07539, 2016. URL\n  Feature Learning. abs/1605.09782, 2016. URL http:                http://arxiv.org/abs/1607.07539.\n //arxiv.org/abs/1605.09782.\n                                                                 Zenati, H., Foo, C. S., Lecouat, B., Manek, G., and Chan-\nDumoulin, V., Belghazi, M. I. D., Poole, B., Lamb, A.,             drasekhar, V. R. Efficient GAN-Based Anomaly Detec-\n Arjovsky, M., Mastropietro, O., and Courville, A. Adver-          tion. abs/1802.06222, 2018. URL http://arxiv.\n  sarially learned inference. 2017. URL http://arXiv.              org/abs/1802.06222.\n  org/abs/1606.00704.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\n Warde-Farley, D., Ozair, S., Courville, A., and Ben-\n  gio, Y. Generative Adversarial Nets. pp. 2672–2680,\n  2014. URL http://papers.nips.cc/paper/\n  5423-generative-adversarial-nets.pdf.\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-\n   to-Image Translation with Conditional Adversarial Net-\n  works. abs/1611.07004, 2016. URL http://arxiv.\n   org/abs/1611.07004.\n\f                                           GANs for Anomaly Detection: a survey\n\nA. Additional Results\nIn this section, we present more detailed results regarding the different combinations of training-testing pipelines of the\nBiGAN/EGBAD and GANomaly architectures on MNIST and Fashion-MNIST; results of the CIFAR10 dataset, trained\nwith GANomaly are presented too. Moreover, the results of the addition of a function (namely, a leaky relu activation\nfunction in the first layer of the encoder) that was missing from inside the EGBAD published code architecture will be\nintroduced. Surprisingly, the tests have led to the result that without that activation function the network performs better.\n\nA.1. Version 1\nBefore getting the results shown in Appendix A.2 we have employed a different test pipeline with a different architecture of\nthe BiGAN network. The main difference was mainly related to:\n\n\n\n  • Testing phase The testing phase is applied twice (for both BiGAN/EGBAD and GANomaly architectures). Once\n    during the model selection during training, and once after the training was finished as a stand alone phase.\n\n\n  • Activation function A Leaky Relu function, erroneously missing from the original EGBAD (Zenati et al., 2018) work\n    has been here inserted after the first convolutional layer of the encoder model.\n\n\n\nA brief visual understanding of the configuration cases for the BiGAN/EGBAD architecture it is shown in Table 4 below.\nThe configuration is similar to the one shown in Table 1 with the difference that, here, the testing phase has been done\nseparately. The same is true for the GANomaly implemented architecture, shown in Table 5.\n\n\n\n\n                                                         Train                         Test\n                                              BCE      FM        Residual         BCE         FM\n                                    Case 1      X                   X                  X\n                                    Case 2      X                   X                         X\n                                    Case 3               X          X                  X\n                                    Case 4               X          X                         X\n                                    Case 5      X                                      X\n                                    Case 6      X                                             X\n                                    Case 7               X                             X\n                                    Case 8               X                                    X\n\n                               Table 4. BiGAN - Training and testing configuration combinations.\n\n\n\n                                                       Train                    Test\n                                                    BCE      FM         BCE            FM\n                                         Case 5      X                      X\n                                         Case 6      X                                 X\n                                         Case 7                X            X\n                                         Case 8                X                       X\n\n                             Table 5. GANomaly - Training and testing configuration combinations.\n\f                                                     GANs for Anomaly Detection: a survey\n\nA.1.1. MNIST\nHere, we present the results on the MNIST dataset. We show in Figure 6 the results of the BiGA/EGBAD architecture and\nin Figure 7 the results of the GANomaly architecture. In the following, MNIST results are depicted.\n\n                                                 MNIST Tests (w and w/o residual loss)\n\n           0.8                                                            brtb               0.8                                               btb\n                                                                          brtf                                                                 btf\n                                                                          frtb                                                                 ftb\n                                                                          frtf                                                                 ftf\n           0.6                                                                               0.6\n   AUPRC\n\n\n\n\n                                                                                     AUPRC\n           0.4                                                                               0.4\n\n\n\n           0.2                                                                               0.2\n\n\n\n                 0   1     2     3     4     5       6      7     8          9                     0   1   2     3    4    5    6      7   8     9\n                               (a) Anomalous classes                                                           (b) Anomalous classes\n\nFigure 6. Performance of version 1 of BiGAN/EGBAD on MNIST measured by the area under the precision-recall curve (AURPC). Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\n\n                                             GANomaly test results on MNIST dataset\n\n                                                          MNIST bce\n                                                          MNIST fm\n                                                 1\n\n                                             0.8\n                                     AUPRC\n\n\n\n\n                                             0.6\n\n                                             0.4\n\n                                             0.2\n\n\n                                                      0      1        2          3     4     5     6   7   8    9\n                                                                          (a) Anomalous classes\n\nFigure 7. GANomaly results on MNIST. All tests are here performed through a bce metric and measured by the area under the precision-\nrecall curve (AURPC). Training is done with a bce loss and a fm loss.\n\f                                                          GANs for Anomaly Detection: a survey\n\nA.1.2. FASHION -MNIST\nHere we present the results obtained on the Fashion-MNIST dataset. In Figure 8 we show the results of BiGAN/EGBAD\narchitectures and in Figure 9 the results of the GANomaly architecture.\n\n                                                 Fashion-MNIST Tests (w and w/o residual loss)\n\n          0.8       brtb                                                                   0.8       btb\n                    brtf                                                                             btf\n                    frtb                                                                             ftb\n                    frtf                                                                             ftf\n          0.6                                                                              0.6\n  AUPRC\n\n\n\n\n                                                                                   AUPRC\n          0.4                                                                              0.4\n\n\n\n          0.2                                                                              0.2\n\n\n\n                0          1   2     3     4      5       6      7     8      9                  0         1   2     3    4    5    6      7   8   9\n                                   (a) Anomalous classes                                                           (b) Anomalous classes\n\nFigure 8. Performance of version 1 of BiGAN/EGBAD on Fashion-MNIST measured by the area under the precision-recall curve\n(AURPC). Best viewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce\n(+ residual loss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce\ntested, ”f(r)tf”: fm (+ residual loss) trained and fm tested\n\n\n                                             GANomaly test results on Fashion-MNIST dataset\n\n                                                               F-MNIST bce\n                                                               F-MNIST fm\n                                                      1\n\n                                                  0.8\n                                         AUPRC\n\n\n\n\n                                                  0.6\n\n                                                  0.4\n\n                                                  0.2\n\n\n                                                           0      1     2      3     4     5     6         7   8    9\n                                                                             (a) Anomalous classes\n\nFigure 9. GANomaly results on Fashion-MNIST. All tests are here performed with a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\f                                                GANs for Anomaly Detection: a survey\n\nA.1.3. CIFAR10\nIn the following Figure 10 we present the results on the CIFAR10 dataset.\n\n                                          GANomaly test results on Cifar10 dataset\n\n                                          0.8        MNIST bce\n                                                     MNIST fm\n\n\n\n                                          0.6\n                                  AUPRC\n\n                                          0.4\n\n\n\n                                          0.2\n\n\n\n                                                 0      1        2     3    4    5    6      7   8   9\n                                                                     (a) Anomalous classes\n\nFigure 10. GANomaly results on CIFAR10. All tests are here performed through a bce metric. Training is done with a bce loss and a fm\nloss.\n\f                                                GANs for Anomaly Detection: a survey\n\nA.2. Version 2\nIn Appendix A.2 we get back to the original version of the EGBAD (Zenati et al., 2018) and find that the results are much\nbetter, even if there is no activation function in the first convolutional layer. The main differences from Appendix A.1 are the\nfollowing:\n\n   • Testing phase The testing phase is applied only once during the model selection during training.\n   • Activation function The Leaky Relu function has here been left out as for the original EGBAD paper(Zenati et al.,\n     2018).\n\nThe training and testing pipelines employed have been already described. Please refer to section 1.3 and section 2.3 for\nmore details. Every following section will introduce the results for a specific dataset.\n\nA.2.1. MNIST\nHere we present the results for the MNIST dataset. In Figure 11 we show the results of BiGAN/EGBAD. In Figure 12 we\nshow the results of GANomaly architecture.\n\n                                               MNIST Tests (w and w/o residual loss)\n\n\n            1                                                                   1\n\n           0.8                                                                 0.8\n   AUPRC\n\n\n\n\n                                                                       AUPRC\n\n\n\n\n           0.6                                                                 0.6\n\n           0.4                                                                 0.4\n                                                                brtb                                                                btb\n                                                                brtf                                                                btf\n           0.2                                                  frtb\n                                                                               0.2                                                  ftb\n                                                                frtf                                                                ftf\n\n\n                 0   1     2     3    4    5     6     7    8      9                 0   1     2     3    4     5    6     7    8     9\n                               (a) Anomalous classes                                               (b) Anomalous classes\n\nFigure 11. Performance of version 2 of BiGAN/EGBAD on MNIST measured by the area under the precision-recall curve (AUPRC). Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\f                                                 GANs for Anomaly Detection: a survey\n\n                                           GANomaly test results on MNIST dataset\n\n                                                      MNIST bce\n                                                      MNIST fm\n                                            1\n\n                                           0.8\n\n\n\n\n                                   AUPRC\n                                           0.6\n\n                                           0.4\n\n                                           0.2\n\n\n                                                  0      1        2     3    4    5    6      7   8   9\n                                                                      (a) Anomalous classes\n\nFigure 12. GANomaly results on MNIST. All tests are here performed through a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\f                                                      GANs for Anomaly Detection: a survey\n\nA.2.2. FASHION -MNIST\nIn the following, we present the plots regarding the performances on Fashion-MNIST dataset. Figure 13 shows the\nperformance of BiGAN/EGBAD model and Figure 14 the performance of GANomaly model.\n\n                                             Fashion-MNIST Tests (w and w/o residual loss)\n\n\n            1                                                                                   1\n\n           0.8                                                                              0.8\n   AUPRC\n\n\n\n\n                                                                                    AUPRC\n           0.6                                                                              0.6\n\n           0.4                                                                              0.4\n                                                                         brtb                                                                   btb\n                                                                         brtf                                                                   btf\n           0.2                                                           frtb\n                                                                                            0.2                                                 ftb\n                                                                         frtf                                                                   ftf\n\n\n                 0   1     2     3     4      5       6      7     8        9                       0   1   2     3    4    5    6      7   8     9\n                               (a) Anomalous classes                                                            (b) Anomalous classes\n\nFigure 13. Performance of version 2 of BiGAN/EGBAD on Fashion-MNIST measured by the area under the precision-recall curve. Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\n\n                                         GANomaly test results on Fashion-MNIST dataset\n\n                                                           F-MNIST bce\n                                                           F-MNIST fm\n                                                  1\n\n                                              0.8\n                                     AUPRC\n\n\n\n\n                                              0.6\n\n                                              0.4\n\n                                              0.2\n\n\n                                                       0      1     2           3     4     5       6   7   8    9\n                                                                         (a) Anomalous classes\n\nFigure 14. GANomaly results on Fashion-MNIST. All tests are here performed through a bce metric and using the area under the\nprecision-recall curve (AURPC). Training is done with a bce loss and a fm loss.\n\f                                                 GANs for Anomaly Detection: a survey\n\nA.2.3. CIFAR10\n                                           GANomaly test results on Cifar10 dataset\n\n                                           0.8        MNIST bce\n                                                      MNIST fm\n\n\n\n                                           0.6\n\n\n                                   AUPRC   0.4\n\n\n\n                                           0.2\n\n\n\n                                                  0      1        2     3    4    5    6      7   8   9\n                                                                      (a) Anomalous classes\n\nFigure 15. GANomaly results on CIFAR10. All tests are here performed through a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\f","page0thumbnail":"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","metadata":{"pdfcpu":{"header":{"creation":"2024-08-23 15:58:30 CEST","version":"pdfcpu v0.8.0 dev"},"infos":[{"author":"Federico Di Mattia, Paolo Galeone, Michele De Simoni, Emanuele Ghelfi","creationDate":"D:20190628004906+00'00'","creator":"LaTeX with hyperref package","keywords":["Machine Learning","ICML","Deep Learning","Anomaly Detection","GANs"],"modificationDate":"D:20190628004906+00'00'","names":true,"pageCount":16,"pageMode":"UseNone","pageSizes":[{"height":792,"width":612}],"producer":"pdfTeX-1.40.17","properties":{"PTEX.Fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"},"source":"/tmp/blobproc-pdf-3449068341.pdf","title":"A Survey on GANs for Anomaly Detection","unit":"points","version":"1.5"}]},"pdfinfo":{"title":"A Survey on GANs for Anomaly Detection","keywords":"Machine Learning, ICML, Deep Learning, Anomaly Detection, GANs","author":"Federico Di Mattia, Paolo Galeone, Michele De Simoni, Emanuele Ghelfi","creator":"LaTeX with hyperref package","producer":"pdfTeX-1.40.17","creation_date":"Fri Jun 28 02:49:06 2019 CEST","mod_date":"Fri Jun 28 02:49:06 2019 CEST","custom_metadata":true,"form":"none","pages":16,"page_size":"612 x 792 pts (letter)","filesize":1215637,"pdf_version":"1.5"}},"pdfextra":{"page0height":792,"page0width":612,"page_count":16,"pdf_version":"1.5"},"weblinks":["http://arXiv","http://arxiv","http://arxiv.org/abs/1607.07539","http://arxiv.org/abs/1805.06725","http://dblp.uni-trier.de/db/journals/","http://kdd.ics.uci.edu/","http://papers.nips.cc/paper/","http://yann.lecun.com/exdb/","https://www.tensorflow.org/"]}
