{"sha1hex":"a93ed137ab87f59cf03a491d097ddd396fa7ebca","status":"success","fileinfo":{"size":1215637,"sha1hex":"a93ed137ab87f59cf03a491d097ddd396fa7ebca","sha256hex":"5da99da592ca16f51dc204ff1ad54070487e231eab57bf120dbf61a6e9a80767","md5hex":"3b5e8d09f374d104f133e6e199e1b16b","mimetype":"application/pdf"},"text":"A Survey on GANs for Anomaly Detection\n\nFederico Di Mattia 1 * Paolo Galeone 1 * Michele De Simoni 1 * Emanuele Ghelfi 1 *\n\narXiv:1906.11632v1 [cs.LG] 27 Jun 2019\n\nAbstract\nAnomaly detection is a significant problem faced\nin several research areas. Detecting and correctly\nclassifying something unseen as anomalous is\na challenging problem that has been tackled in\nmany different manners over the years. Generative Adversarial Networks (GANs) and the adversarial training process have been recently employed to face this task yielding remarkable results. In this paper we survey the principal GANbased anomaly detection methods, highlighting\ntheir pros and cons. Our contributions are the\nempirical validation of the main GAN models\nfor anomaly detection, the increase of the experimental results on different datasets and the public\nrelease of a complete Open Source toolbox for\nAnomaly Detection using GANs.\n\n1. Introduction\nAnomalies are patterns in data that do not conform to a\nwell-defined notion of normal behavior (Chandola et al.,\n2009). Generative Adversarial Networks (GANs) and the\nadversarial training framework (Goodfellow et al., 2014)\nhave been successfully applied to model complex and high\ndimensional distribution of real-world data. This GAN characteristic suggests they can be used successfully for anomaly\ndetection, although their application has been only recently\nexplored. Anomaly detection using GANs is the task of\nmodeling the normal behavior using the adversarial training\nprocess and detecting the anomalies measuring an anomaly\nscore (Schlegl et al., 2017). To the best of our knowledge,\nall the GAN-based approaches to anomaly detection build\nupon on the Adversarial Feature Learning idea (Donahue\net al., 2016) in which the BiGAN architecture has been\nproposed. In their original formulation, the GAN framework learns a generator that maps samples from an arbitrary\nlatent distribution (noise prior) to data as well as a discriminator which tries to distinguish between real and generated\nsamples. The BiGAN architecture extended the original for*\nEqual contribution 1 Zuru Tech, Modena, Italy. Correspondence to: Federico Di Mattia \u003cfederico.d@zuru.tech\u003e.\n\nmulation, adding the learning of the inverse mapping which\nmaps the data back to the latent representation. A learned\nfunction that maps input data to its latent representation together with a function that does the opposite (the generator)\nis the basis of the anomaly detection using GANs.\nThe paper is organized as follows. In Section 1 we introduce\nthe GANs framework and, briefly, its most innovative extensions, namely conditional GANs and BiGAN, respectively\nin Section 1.2 and Section 1.3. Section 2 contains the state\nof the art architectures for anomaly detection with GANs.\nIn Section 3 we empirically evaluate all the analyzed architectures. Finally, Section 4 contains the conclusions and\nfuture research directions.\n1.1. GANs\nGANs are a framework for the estimation of generative\nmodels via an adversarial process in which two models, a\ndiscriminator D and a generator G, are trained simultaneously. The generator G aim is to capture the data distribution, while the discriminator D estimates the probability that\na sample came from the training data rather than G. To learn\na generative distribution pg over the data x the generator\nbuilds a mapping from a prior noise distribution pz to a data\nspace as G(z; θG ), where θG are the generator parameters.\nThe discriminator outputs a single scalar representing the\nprobability that x came from real data rather than from pg .\nThe generator function is denoted with D(x; θD ), where θD\nare discriminator parameters.\nThe original GAN framework (Goodfellow et al., 2014)\nposes this problem as a min-max game in which the two\nplayers (G and D) compete against each other, playing the\nfollowing zero-sum min-max game:\nmin max V (D, G) = Ex∼pdata (x) [log D(x)]+\nG\n\nD\n\n(1)\n\nEz∼pz (z) [log (1 − D(G(z)))] .\n\n1.2. Conditional GANs\nGANs can be extended to a conditional model (Mirza \u0026\nOsindero, 2014) conditioning either G or D on some extra information y. The y condition could be any auxiliary\ninformation, such as class labels or data from other modalities. We can perform the conditioning by feeding y into\n\n\fGANs for Anomaly Detection: a survey\n\nFigure 1. The structure of BiGAN proposed in (Donahue et al., 2016).\n\nboth the discriminator and generator as an additional input\nlayer. The generator combines the noise prior pz (z) and\ny in a joint hidden representation, the adversarial training\nframework allows for considerable flexibility in how this\nhidden representation is composed. In the discriminator, x\nand y are presented as inputs to a discriminative function.\nThe objective function considering the condition becomes:\n\nmin max V (D, G) = Ex∼pdata (x|y) [log D(x)]+\nG\n\nD\n\n(2)\n\nEz∼pz (z) [log(1 − D(G(z|y)))].\n\nperformance issues of AnoGAN a BiGAN-based approach\nhas been proposed in Zenati et al. (2018), here referred as\nEGBAD (Efficient GAN Based Anomaly Detection), that\noutperformed AnoGAN execution time. Recently, Akcay\net al. (2018) advanced a GAN + autoencoder based approach\nthat exceeded EGBAD performance from both evaluation\nmetrics and execution speed.\nIn the following sections, we present an analysis of the\nconsidered architecture. The term sample and image are\nused interchangeably since GANs can be used to detect\nanomalies on a wide range of domains, but all the analyzed\narchitectures focused mostly on images.\n\n1.3. BiGAN\n2.1. AnoGAN\nBidirectional GAN (Donahue et al., 2016) extends the GAN\nframework including an encoder E(x; θE ) that learns the\ninverse of the generator E = G−1 . The BiGAN training\nprocess allows learning a mapping simultaneously from\nlatent space to data and vice versa. The encoder E is a\nnon-linear parametric function in the same way as G and\nD, and it can be trained using gradient descent. As in the\nconditional GANs scenario, the discriminator must learn to\nclassify not only real and fake samples, but pairs in the form\n(G(z), z) or (x, E(x)). The BiGAN training objective is:\n\nmin max V (D, G, E) =\nG,E\n\nAnoGAN aim is to use a standard GAN, trained only on\npositive samples, to learn a mapping from the latent space\nrepresentation z to the realistic sample x̂ = G(z) and use\nthis learned representation to map new, unseen, samples\nback to the latent space. Training a GAN on normal samples\nonly, makes the generator learn the manifold X of normal\nsamples. Given that the generator learns how to generate\nnormal samples, when an anomalous image is encoded its\nreconstruction will be non-anomalous; hence the difference\nbetween the input and the reconstructed image will highlight\nthe anomalies. The two steps of training and detecting\nanomalies are summarized in Figure 2.\n\nD\n\nEx∼pdata (x) [Ez∼pE(z|x) [log D(x, z)]]+\n\n(3)\n\nEz∼pz (z) [Ex∼pG (x|z) [log(1 − D(x, z)))]].\nFigure 1 depicts a visual structure of the BiGAN architecture.\n\n2. GANs for anomaly detection\nAnomaly detection using GANs is an emerging research\nfield. Schlegl et al. (2017), here referred to as AnoGAN,\nwere the first to propose such a concept. In order to face the\n\nThe authors have defined the mapping of input samples to\nthe latent space as an iterative process. The aim is to find a\npoint z in the latent space that corresponds to a generated\nvalue G(z) that is similar to the query value x located on\nthe manifold X of the positive samples. The research process is defined as the minimization trough γ = 1, 2, . . . , Γ\nbackpropagation steps of the loss function defined as the\nweighted sum of the residual loss LR and discriminator loss\nLD , in the spirit of Yeh et al. (2016).\nThe residual loss measures the dissimilarity between the\nquery sample and the generated sample in the input domain\n\n\fGANs for Anomaly Detection: a survey\n\nFigure 2. AnoGAN (Schlegl et al., 2017). The GAN is trained on positive samples. At test time, after Γ research iteration the latent vector\nthat maps the test image to its latent representation is found zΓ . The reconstructed image G(zΓ ) is used to localize the anomalous regions.\n\nspace:\nLR (zγ ) = ||x − G(zγ )||1 .\n\n(4)\n\nThe discriminator loss takes into account the discriminator\nresponse. It can be formalized in two different ways. Following the original idea of Yeh et al. (2016), hence feeding the\ngenerated image G(zγ ) into the discriminator and calculating the sigmoid cross-entropy as in the adversarial training\nphase: this takes into account the discriminator confidence\nthat the input sample is derived by the real data distribution.\nAlternatively, using the idea introduced by Salimans et al.\n(2016), and used by the AnoGAN (Schlegl et al., 2017)\nauthors, to compute the feature matching loss, extracting\nfeatures from a discriminator layer f in order to take into\naccount if the generated sample has similar features of the\ninput one, by computing:\n\n• Used the same mapping scheme to define an anomaly\nscore.\nCons\n• Requires Γ optimization steps for every new input: bad\ntest-time performance.\n• The GAN objective has not been modified to take into\naccount the need for the inverse mapping learning.\n• The anomaly score is difficult to interpret, not being in\nthe probability range.\n2.2. EGBAD\n\nA(x) has no upper bound; to high values correspond an\nhigh probability of x to be anomalous.\n\nEfficient GAN-Based Anomaly Detection (EGBAD) (Zenati\net al., 2018) brings the BiGAN architecture to the anomaly\ndetection domain. In particular, EGBAD tries to solve the\nAnoGAN disadvantages using Donahue et al. (2016) and Dumoulin et al. (2017) works that allows learning an encoder\nE able to map input samples to their latent representation\nduring the adversarial training. The importance of learning\nE jointly with G is strongly emphasized, hence Zenati et al.\n(2018) adopted a strategy similar to the one indicated in\nDonahue et al. (2016) and Dumoulin et al. (2017) in order\nto try to solve, during training, the optimization problem\nminG,E maxD V (D, E, G) where V (D, E, G) is defined\nas in Equation 3. The main contribution of the EGBAD is\nto allow computing the anomaly score without Γ optimization steps during the inference as it happens in AnoGAN\n(Schlegl et al., 2017).\n\nIt should be noted that the minimization process is required\nfor every single input sample x.\n\n2.3. GANomaly\n\nLD (zγ ) = ||f (x) − f (G(zγ ))||1 ,\n\n(5)\n\nhence the proposed loss function is:\nL(zγ ) = (1 − λ) · LR (zγ ) + γ · LD (zγ ).\n\n(6)\n\nIts value at the Γ-th step coincides with the anomaly score\nformulation:\nA(x) = L(zΓ ).\n\n(7)\n\n2.1.1. P ROS AND CONS\nPros\n• Showed that GANs can be used for anomaly detection.\n• Introduced a new mapping scheme from latent space\nto input data space.\n\nAkcay et al. (2018) introduce the GANomaly approach. Inspired by AnoGAN (Schlegl et al., 2017), BiGAN (Donahue\net al., 2016) and EGBAD (Zenati et al., 2018) they train a\ngenerator network on normal samples to learn their manifold X while at the same time an autoencoder is trained to\nlearn how to encode the images in their latent representation efficiently. Their work is intended to improve the ideas\nof Schlegl et al. (2017), Donahue et al. (2016) and Zenati\n\n\fGANs for Anomaly Detection: a survey\n\nReal / Fake\n\nInput/Output\n\nConv\n\nLeakyReLU\n\nBatchNorm\n\nConvTranspose\n\nReLU\n\nTanh\n\nSoftmax\n\nFigure 3. GANomaly architecture and loss functions from (Akcay et al., 2018).\n\net al. (2018). Their approach only needs a generator and a\ndiscriminator as in a standard GAN architecture.\n\nto Figure 3 for a visual representation of the architecture\nunderpinning GANomaly.\n\nGenerator network The generator network consists of\nthree elements in series, an encoder GE a decoder GD\n(both assembling an autoencoder structure) and another encoder E. The architecture of the two encoders is the same.\nGE takes in input an image x and outputs an encoded version z of it. Hence, z is the input of GD that outputs x̂,\nthe reconstructed version of x. Finally, x̂ is given as an\ninput to the encoder E that produces ẑ. There are two main\ncontributions from this architecture. First, the operating\nprinciple of the anomaly detection of this work lies in the\nautoencoder structure. Given that we learn to encode normal (non-anomalous) data (producing z) and given that we\nlearn to generate normal data (x̂) starting from the encoded\nrepresentation z, when the input data x is an anomaly its\nreconstruction will be normal. Because the generator will always produce a non-anomalous image, the visual difference\nbetween the input x and the produced x̂ will be high and in\nparticular will spatially highlight where the anomalies are\nlocated. Second, the encoder E at the end of the generator\nstructure helps, during the training phase, to learn to encode\nthe images in order to have the best possible representation\nof x that could lead to its reconstruction x̂.\n\nThe GANomaly architecture differs from AnoGAN (Schlegl\net al., 2017) and from EGBAD (Zenati et al., 2018). In\nFigure 4 the three architectures are presented.\n\nDiscriminator network The discriminator network D is\nthe other part of the whole architecture, and it is, with the\ngenerator part, the other building block of the standard GAN\narchitecture. The discriminator, in the standard adversarial\ntraining, is trained to discern between real and generated\ndata. When it is not able to discern among them, it means\nthat the generator produces realistic images. The generator\nis continuously updated to fool the discriminator. Refer\n\nBeside these two networks, the other main contribution of\nGANomaly is the introduction of the generator loss as the\nsum of three different losses; the discriminator loss is the\nclassical discriminator GAN loss.\nGenerator loss The objective function is formulated by\ncombining three loss functions, each of which optimizes a\ndifferent part of the whole architecture.\nAdversarial Loss The adversarial loss it is chosen to be the\nfeature matching loss as introduced in Schlegl et al. (2017)\nand pursued in Zenati et al. (2018):\nLadv = Ex∼pX ||f (x) − Ex∼pX f (G(x))||2 ,\n\n(8)\n\nwhere f is a layer of the discriminator D, used to extract\na feature representation of the input . Alternatively, binary\ncross entropy loss can be used.\nContextual Loss Through the use of this loss the generator learns contextual information about the input data. As\nshown in (Isola et al., 2016) the use of the L1 norm helps to\nobtain better visual results:\nLcon = Ex∼pX ||x − G(x)||1 .\n\n(9)\n\nEncoder Loss This loss is used to let the generator network\nlearn how to best encode a normal (non-anomalous) image:\n\n\fGANs for Anomaly Detection: a survey\n\nz\n\nG(z)\n\nx'\n\nx\n\nD(x, x')\n\nA\n\nz\n\nG(z)\n\nz'\n\nE(x)\n\nx'\n\nx\n\nx GE(x) z\n\nGD(z)\n\nx' E(x') z'\n\nD(x, x')\n\nD(x, x')\n\nB\n\nreal/fake\n\nC\n\nFigure 4. Architectures comparison. A) AnoGAN (Schlegl et al., 2017), B) EGBAD (Zenati et al., 2018), C) GANomaly (Akcay et al.,\n2018).\n\n• The contextual loss can be used to localize the anomaly.\nLenc = Ex∼pX ||GE (x) − E(G(x))||2 .\n\n(10)\n\nThe resulting generator loss will be the result of the\n(weighted) sum of the three previously defined losses:\nL = wadv Ladv + wcon Lcon + wenc Lenc ,\n\n(11)\n\nwhere wadv , wcon and wenc are weighting parameters used\nto adjust the importance of the three losses.\nAt test stage, the authors proposed to compute the anomaly\nscore in a different way respect to AnoGAN: only using\nusing Lenc :\n\nCons\n• It allows to detect anomalies both in the image space\nand in the latent space, but the results couldn’t match:\na higher anomaly score, that’s computed only in the\nlatent space, can be associated with a generated sample\nwith a low contextual loss value and thus very similar\nto the input - and vice versa.\n• Defines a new anomaly score.\n\n3. Experiments\nA(x) = ||GE (x) − E(G(x))||2 .\n\n(12)\n\nIn order to make the anomaly score easier to interpret, they\nproposed to compute the anomaly score for every sample x̂\nin thentest set D̂, getting a o\nset of individual anomaly scores:\nS = si : A(x̂i ) , x̂i ∈ D̂ and then apply feature scaling\nto have the anomaly scores within the probabilistic range of\n[0, 1]:\ns0i =\n\nsi + min(S)\n.\nmax(S) − min(S)\n\n(13)\n\n2.3.1. P ROS AND CONS\nPros\n• An encoder is learned during the training process,\nhence we don’t have the need for a research process as\nin AnoGAN (Schlegl et al., 2017).\n• Using an autoencoder like architecture (no use of noise\nprior) makes the entire learning process faster.\n• The anomaly score is easier to interpret.\n\nTo evaluate the performance of every Anomaly Detection\nalgorithm described we re-implemented all of them using\nthe popular deep learning framework Tensorflow (Abadi\net al., 2015) creating a publicly available toolbox for the\nAnomaly Detection with GANs.\nExperiments were made trying to improve, where possible\n(i.e., where there were known errors communicated directly\nby the authors), the code. The results shown in the following\nsections are the best empirically obtained among all the\ntests carried out and do not necessarily derive from the\nconfigurations of the networks that at a theoretical level\nshould be the correct ones. In particular, as described in\nAppendix A, in version 1 (Appendix A.1) of our tests, with\nthe correct implementation of the BiGAN/EGBAD models,\nwe obtained worse results than those obtained in version 2\n(Appendix A.2) of our tests with the architecture already\nimplemented in the reference paper, although this contained\nknown errors.\n3.1. Experimental setup\nThe experiments are performed using a Intel R Core\n7820X CPU and NVIDIA R GTX 1080 Ti.\n\nTM\n\ni7-\n\n\fGANs for Anomaly Detection: a survey\n\nBiGAN/EGBAD and GANomaly performance comparison on MNIST and Fashion-MNIST datasets\nBiGAN/EGBAD\n\n1\n\nGANomaly\n\n0.8\nAUPRC\n\n0.8\nAUPRC\n\nBiGAN/EGBAD\n\n1\n\nGANomaly\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) MNIST anomalous classes\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(b) Fashion-MNIST anomalous classes\n\nFigure 5. Performance on MNIST (a) and Fashion-MNIST (b) of BiGAN/EGBAD and GANomaly models measured by the area under\nthe precision-recall curve (AUPRC). The results have been selected as the best result between different type of training (with bce/fm and\nwith residual loss). Best viewed in color.\n\n3.1.1. DATASETS\nWe decided to train and test on widely known datasets commonly found in the literature: MNIST (LeCun \u0026 Cortes,\n2010a), FashionMNIST (Xiao et al., 2017), CIFAR-10\n(Krizhevsky et al., 2007), and KDD99 (Lichman, 1999).\nMNIST: MNIST dataset (LeCun \u0026 Cortes, 2010b) is a\ndatabase of handwritten digits. It consists of 28 × 28 pixels\ngrayscale images split in a training set of 60,000 examples\nand a test set of 10,000 examples. The output classes are 10\nin total and represent the ten digits from 0 to 9. In order to\nreplicate the results of the papers presented in this work, the\ntraining process and the subsequent testing phase have been\nevaluated on each class, i.e., one at a time, only one class\nhas been considered as the anomaly class, and the remaining\nnine classes have been considered together as the normal\ndata.\nFashion MNIST: Fashion-MNIST dataset (Xiao et al.,\n2017) is a dataset intended to be a more complex dropin replacement for the traditional MNIST dataset, developed\nby Zalando R Research Group. It shares with the MNIST\ndataset the same image size and train-test split; it consists\nof 28 × 28 pixels images split in a training set of 60,000\nexamples and a test set of 10,000 examples. Each image is\na grayscale image associated, as the MNIST dataset, with a\nlabel from 10 classes, from 0 to 9. Each example represents\na different item of clothing. As for the MNIST dataset, the\ntraining and the following test phase were conducted on\neach class, one at a time.\nCIFAR-10: A dataset of tiny images of various subjects,\n\nCIFAR-10 consists of 60000 32 × 32 color images in 10\nclasses, with 6000 images per class. There are 50000 training images and 10000 test images.\nKDD: KDD dataset (Lichman, 1999) consists in a collection of network intrusion detection data, this is the data set\nused for The Third International Knowledge Discovery and\nData Mining Tools Competition held in conjunction with\nKDD-99 The Fifth International Conference on Knowledge\nDiscovery and Data Mining. The competition task was to\nbuild a network intrusion detector, a predictive model capable of distinguishing between “bad” connections, called\nintrusions or attacks, and “good” normal connections. All\nthe examples are string tuples or numbers. Given the considerable amount of data inside the data set, we used as in\n(Zenati et al., 2018), the KDDCUP99 10 percent version of\nthe data set. Since the anomalies are in a higher percentage\nthan normal data, the normal ones have been considered as\nanomalies.\n3.1.2. M ETHODOLOGY\nWe follow the same methodology employed by Zenati et al.\nin the official code accompanying Zenati et al. (2018).\nWe start by getting all the dataset together (train and test\nsplit). Starting from this one big pool of examples, we\nchoose one class as an anomaly and, after shuffling the\ndataset, we then create a training set by using 80% of the\nwhole data while the remaining 20% is used for the testing\nset, this process is repeated for all the classes in the dataset.\nEach model is trained accordingly to its original implementation on the training set and is tested on the whole dataset\n\n\fGANs for Anomaly Detection: a survey\n\nTrain\nBCE\nCase 1\nCase 2\nCase 3\nCase 4\n\nTest\n\nTrain\n\nFM\n\nResidual\n\nBCE\n\nFM\n\nX\n\nX\nX\n\nX\nX\nX\nX\n\nX\nX\nX\nX\n\nX\nX\nX\n\nBCE\nCase 1\nCase 2\n\nTest\n\nFM\n\nBCE\n\nFM\n\nX\n\nX\nX\n\nX\nX\n\nX\n\nTable 2. GANomaly - Training and testing configuration combinations.\n\nTable 1. BiGAN - Training and testing configuration combinations.\n\nKDD\nmade up of both regular and anomalous data.\n3.2. Results\nAll tests have been made measuring the area under precision\nand recall curve (AUPRC). For a complete understanding of\nall the results, please refer to Appendix A. Our fully staticTensorflow implementation (i.e., only Tensorflow framework has been used, we relied upon neither Keras nor eager\nexecution) produced different results from the one depicted\nin the original Donahue et al. (2016), Schlegl et al. (2017),\nand Akcay et al. (2018) papers. The tests have been made by\ntraining the GAN networks with different hyper-parameters\nconfigurations in order to test a broader range of models\nconfigurations. Moreover, following the GANomaly approach, we have added an evaluation process to make model\nselection and thus select the best model during the training\nphase. The model selection has been made for BiGAN and\nGANomaly, since doing it for AnoGAN would be unfeasible, due to the Γ research steps required for every sample\nat test-time. We intentionally left out the performance evaluation of the AnoGAN model. Due to the inner workings\nof the architecture that should have required a very long\ntime to be tested because of the necessity to find, every time\nand for every image taken into consideration, the best latent\nrepresentation, meaning that for every image we would need\n500 training step (500 it is an empirical value found in the\noriginal paper (Schlegl et al., 2017)). Following, the training\nand test methods are described. A summary of the results\nis present in Figure 5 for the MNIST and Fashion-MNIST\ndataset, and in Table 3 for the KDD results.\nBiGAN/EGBAD: With BiGAN/EGBAD (Zenati et al.,\n2018) architecture we have executed the highest number\nof training and testing configurations, this means we have\nperformed, for every label the combinations depicted in Table 1. To better understand what the table shows, taking\nthe first row (Case 1) as an example, we are describing the\ncase where, during the training phase, a Binary Cross Entropy loss (BCE) in combination with a Residual loss has\nbeen used and, during the testing phase, we computed the\nanomaly score twice, using the feature matching (FM) and\nthe BCE. All the other cases should be clear. In particular,\nduring the training phase, the choice between using BCE or\n\nBiGAN/EGBAD\nGANomaly\n\nPrecision\n\nRecall\n\nF1-Score\n\n0.941174\n0.830256\n\n0.956155\n0.841112\n\n0.948605\n0.835648\n\nTable 3. The performances of BiGAN/EGBAD and GANomaly\nmodels on the KDD dataset.\n\nFM influences only the generator loss. The residual loss that\nit is intended as the difference between the original image\nand the image reconstructed by the generator starting from\na latent representation (please refer to Equation (4)) when\nused has been added as an additional term to the BCE loss\nof the encoder. In Figure 5 is possible to see the results on\nthe MNIST and Fashion-MNIST datasets. In Appendix A\nit is possible to see the complete results. The images used\nare the original 28 × 28 images. Any additional result from\nmultiple different tests performed with different random\nseeds has been omitted for the sake of clarity.\nGANomaly: GANomaly training and testing have been\ndone similarly to the previously described BiGAN/EGBAD\narchitecture. The whole procedure is leaner here, with only\nthe combinations shown in Table 2.\nWe added a step of model selection during the training phase\nin order to always save the very best model. For this architecture, the testing phase has been done using an anomaly\nscore equal to the squared difference between the latent\nrepresentations of the image encoded first with autoencoder\npart of the network and, after being reconstructed, encoded\nagain with the encoder. To briefly review the working of\nGANomaly, see Figure 3. The images used are the original\nones resized to 32 × 32. Refer to the plot on the right of\nFigure 5 to a brief summary of the results on MNIST and\nFashion-MNIST datasets and on Table 3 for the results on\nKDD dataset.\nAnoGAN: As previously stated, the results of AnoGAN\nhave not been reproduced due to the onerous computational\nrequirements causing the train and test phases to be intensively time-consuming. The reader could refer to the\noriginal paper (Schlegl et al., 2017) to an overview of the\nresults.\n\n\fGANs for Anomaly Detection: a survey\n\n4. Conclusions\nWe implemented and evaluated the state-of-the-art algorithms for anomaly detection based on GANs. In this work,\nwe unified the three major works under the same framework\n(Tensorflow) and within the same code-base. The analysis\nand implementation of the algorithms allowed us to verify the effectiveness of the GANs-based approach to the\nanomaly detection problem and, at the same time, highlight\nthe differences between the original papers and the publicly\navailable code. Besides, to test the feasibility of the architectures mentioned above, this work intends to provide a\nmodular and ready-to-go solution for everyone desiring an\nanomaly detection toolkit working out of the box.\n\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,\nM., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,\nI., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVigas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,\nM., Yu, Y., and Zheng, X. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015. URL\nhttps://www.tensorflow.org/. Software available from tensorflow.org.\nAkcay, S., Abarghouei, A. A., and Breckon, T. P.\nGANomaly: Semi-Supervised Anomaly Detection via\nAdversarial Training. abs/1805.06725, 2018. URL\nhttp://arxiv.org/abs/1805.06725.\nChandola, V., Banerjee, A., and Kumar., V. Anomaly detection: A survey. 41(3), 2009.\nDonahue, J., Krhenbhl, P., and Darrell, T. Adversarial\nFeature Learning. abs/1605.09782, 2016. URL http:\n//arxiv.org/abs/1605.09782.\nDumoulin, V., Belghazi, M. I. D., Poole, B., Lamb, A.,\nArjovsky, M., Mastropietro, O., and Courville, A. Adversarially learned inference. 2017. URL http://arXiv.\norg/abs/1606.00704.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative Adversarial Nets. pp. 2672–2680,\n2014. URL http://papers.nips.cc/paper/\n5423-generative-adversarial-nets.pdf.\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Imageto-Image Translation with Conditional Adversarial Networks. abs/1611.07004, 2016. URL http://arxiv.\norg/abs/1611.07004.\n\nKrizhevsky, A., Nair, V., and Hinton, G. CIFAR-10 (Canadian Institute for Advanced Research). 2007. URL http:\n//www.cs.toronto.edu/˜kriz/cifar.html.\nLeCun, Y. and Cortes, C. MNIST handwritten digit database.\n2010a. URL http://yann.lecun.com/exdb/\nmnist/.\nLeCun, Y. and Cortes, C. MNIST handwritten digit database.\n2010b. URL http://yann.lecun.com/exdb/\nmnist/.\nLichman, M. KDD Cup 1999 Data (University of California\nIrvine). 1999. URL http://kdd.ics.uci.edu/\ndatabases/kddcup99/kddcup99.html.\nMirza, M. and Osindero, S. Conditional Generative Adversarial Nets. abs/1411.1784, 2014. URL http:\n//arxiv.org/abs/1411.1784.\nSalimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V.,\nRadford, A., and Chen, X. Improved Techniques for\nTraining GANs. abs/1606.03498, 2016. URL http:\n//arxiv.org/abs/1606.03498.\nSchlegl, T., Seebck, P., Waldstein, S. M., Schmidt-Erfurth,\nU., and Langs, G. Unsupervised Anomaly Detection with\nGenerative Adversarial Networks to Guide Marker Discovery. abs/1703.05921, 2017. URL http://arxiv.\norg/abs/1703.05921.\nXiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST:\nA Novel Image Dataset for Benchmarking Machine\nLearning Algorithms. abs/1708.07747, 2017. URL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1708.html#abs-1708-07747.\nYeh, R. A., Chen, C., Lim, T.-Y., Hasegawa-Johnson, M.,\nand Do, M. N. Semantic Image Inpainting with Perceptual and Contextual Losses. abs/1607.07539, 2016. URL\nhttp://arxiv.org/abs/1607.07539.\nZenati, H., Foo, C. S., Lecouat, B., Manek, G., and Chandrasekhar, V. R. Efficient GAN-Based Anomaly Detection. abs/1802.06222, 2018. URL http://arxiv.\norg/abs/1802.06222.\n\n\fGANs for Anomaly Detection: a survey\n\nA. Additional Results\nIn this section, we present more detailed results regarding the different combinations of training-testing pipelines of the\nBiGAN/EGBAD and GANomaly architectures on MNIST and Fashion-MNIST; results of the CIFAR10 dataset, trained\nwith GANomaly are presented too. Moreover, the results of the addition of a function (namely, a leaky relu activation\nfunction in the first layer of the encoder) that was missing from inside the EGBAD published code architecture will be\nintroduced. Surprisingly, the tests have led to the result that without that activation function the network performs better.\nA.1. Version 1\nBefore getting the results shown in Appendix A.2 we have employed a different test pipeline with a different architecture of\nthe BiGAN network. The main difference was mainly related to:\n\n• Testing phase The testing phase is applied twice (for both BiGAN/EGBAD and GANomaly architectures). Once\nduring the model selection during training, and once after the training was finished as a stand alone phase.\n• Activation function A Leaky Relu function, erroneously missing from the original EGBAD (Zenati et al., 2018) work\nhas been here inserted after the first convolutional layer of the encoder model.\n\nA brief visual understanding of the configuration cases for the BiGAN/EGBAD architecture it is shown in Table 4 below.\nThe configuration is similar to the one shown in Table 1 with the difference that, here, the testing phase has been done\nseparately. The same is true for the GANomaly implemented architecture, shown in Table 5.\n\nTrain\nBCE\nCase 1\nCase 2\nCase 3\nCase 4\nCase 5\nCase 6\nCase 7\nCase 8\n\nTest\n\nFM\n\nResidual\n\nBCE\nX\n\nX\nX\n\nX\nX\nX\nX\n\nX\nX\n\nFM\nX\n\nX\nX\n\nX\nX\n\nX\nX\nX\nX\n\nX\nX\n\nTable 4. BiGAN - Training and testing configuration combinations.\n\nTrain\nBCE\nCase 5\nCase 6\nCase 7\nCase 8\n\nTest\n\nFM\n\nX\nX\n\nBCE\n\nFM\n\nX\nX\nX\nX\n\nX\nX\n\nTable 5. GANomaly - Training and testing configuration combinations.\n\n\fGANs for Anomaly Detection: a survey\n\nA.1.1. MNIST\nHere, we present the results on the MNIST dataset. We show in Figure 6 the results of the BiGA/EGBAD architecture and\nin Figure 7 the results of the GANomaly architecture. In the following, MNIST results are depicted.\nMNIST Tests (w and w/o residual loss)\n0.8\n\n0.8\n\nbrtb\n\nbtb\n\nbrtf\n\nbtf\n\nfrtb\n\nftb\n\nfrtf\n\nftf\n\n0.6\nAUPRC\n\nAUPRC\n\n0.6\n\n0.4\n\n0.2\n\n0.4\n\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n(a) Anomalous classes\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(b) Anomalous classes\n\nFigure 6. Performance of version 1 of BiGAN/EGBAD on MNIST measured by the area under the precision-recall curve (AURPC). Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\nGANomaly test results on MNIST dataset\nMNIST bce\nMNIST fm\n\n1\n\nAUPRC\n\n0.8\n0.6\n0.4\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 7. GANomaly results on MNIST. All tests are here performed through a bce metric and measured by the area under the precisionrecall curve (AURPC). Training is done with a bce loss and a fm loss.\n\n\fGANs for Anomaly Detection: a survey\n\nA.1.2. FASHION -MNIST\nHere we present the results obtained on the Fashion-MNIST dataset. In Figure 8 we show the results of BiGAN/EGBAD\narchitectures and in Figure 9 the results of the GANomaly architecture.\nFashion-MNIST Tests (w and w/o residual loss)\n0.8\n\n0.8\n\nbrtb\n\nbtb\n\nbrtf\n\nbtf\n\nfrtb\n\nftb\n\nfrtf\n\nftf\n\n0.6\nAUPRC\n\nAUPRC\n\n0.6\n\n0.4\n\n0.2\n\n0.4\n\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n(a) Anomalous classes\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(b) Anomalous classes\n\nFigure 8. Performance of version 1 of BiGAN/EGBAD on Fashion-MNIST measured by the area under the precision-recall curve\n(AURPC). Best viewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce\n(+ residual loss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce\ntested, ”f(r)tf”: fm (+ residual loss) trained and fm tested\n\nGANomaly test results on Fashion-MNIST dataset\nF-MNIST bce\nF-MNIST fm\n\n1\n\nAUPRC\n\n0.8\n0.6\n0.4\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 9. GANomaly results on Fashion-MNIST. All tests are here performed with a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\n\fGANs for Anomaly Detection: a survey\n\nA.1.3. CIFAR10\nIn the following Figure 10 we present the results on the CIFAR10 dataset.\nGANomaly test results on Cifar10 dataset\n0.8\n\nMNIST bce\nMNIST fm\n\nAUPRC\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 10. GANomaly results on CIFAR10. All tests are here performed through a bce metric. Training is done with a bce loss and a fm\nloss.\n\n\fGANs for Anomaly Detection: a survey\n\nA.2. Version 2\nIn Appendix A.2 we get back to the original version of the EGBAD (Zenati et al., 2018) and find that the results are much\nbetter, even if there is no activation function in the first convolutional layer. The main differences from Appendix A.1 are the\nfollowing:\n• Testing phase The testing phase is applied only once during the model selection during training.\n• Activation function The Leaky Relu function has here been left out as for the original EGBAD paper(Zenati et al.,\n2018).\nThe training and testing pipelines employed have been already described. Please refer to section 1.3 and section 2.3 for\nmore details. Every following section will introduce the results for a specific dataset.\nA.2.1. MNIST\nHere we present the results for the MNIST dataset. In Figure 11 we show the results of BiGAN/EGBAD. In Figure 12 we\nshow the results of GANomaly architecture.\n\n1\n\n1\n\n0.8\n\n0.8\nAUPRC\n\nAUPRC\n\nMNIST Tests (w and w/o residual loss)\n\n0.6\n0.4\n\n0.6\n0.4\n\nbrtb\nbrtf\n\n0.2\n\nfrtb\n\nbtb\nbtf\n\n0.2\n\nftb\n\nfrtf\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n(a) Anomalous classes\n\n7\n\n8\n\n9\n\nftf\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(b) Anomalous classes\n\nFigure 11. Performance of version 2 of BiGAN/EGBAD on MNIST measured by the area under the precision-recall curve (AUPRC). Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\n\fGANs for Anomaly Detection: a survey\n\nGANomaly test results on MNIST dataset\nMNIST bce\nMNIST fm\n\n1\n\nAUPRC\n\n0.8\n0.6\n0.4\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 12. GANomaly results on MNIST. All tests are here performed through a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\n\fGANs for Anomaly Detection: a survey\n\nA.2.2. FASHION -MNIST\nIn the following, we present the plots regarding the performances on Fashion-MNIST dataset. Figure 13 shows the\nperformance of BiGAN/EGBAD model and Figure 14 the performance of GANomaly model.\n\n1\n\n1\n\n0.8\n\n0.8\nAUPRC\n\nAUPRC\n\nFashion-MNIST Tests (w and w/o residual loss)\n\n0.6\n0.4\n\n0.6\n0.4\n\nbrtb\n\nbtb\n\nbrtf\n\n0.2\n\nbtf\n\n0.2\n\nfrtb\n\nftb\n\nfrtf\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nftf\n\n0\n\n9\n\n1\n\n2\n\n(a) Anomalous classes\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(b) Anomalous classes\n\nFigure 13. Performance of version 2 of BiGAN/EGBAD on Fashion-MNIST measured by the area under the precision-recall curve. Best\nviewed in color. Image (a) depicts the results using the residual loss, image (b) it is without the residual loss. ”b(r)tb”: bce (+ residual\nloss) trained and bce tested, ”b(r)tf”: bce (+ residual loss) trained and fm tested, ”f(r)tb”: fm (+ residual loss) trained and bce tested,\n”f(r)tf”: fm (+ residual loss) trained and fm tested\n\nGANomaly test results on Fashion-MNIST dataset\nF-MNIST bce\nF-MNIST fm\n\n1\n\nAUPRC\n\n0.8\n0.6\n0.4\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 14. GANomaly results on Fashion-MNIST. All tests are here performed through a bce metric and using the area under the\nprecision-recall curve (AURPC). Training is done with a bce loss and a fm loss.\n\n\fGANs for Anomaly Detection: a survey\n\nA.2.3. CIFAR10\nGANomaly test results on Cifar10 dataset\n0.8\n\nMNIST bce\nMNIST fm\n\nAUPRC\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n(a) Anomalous classes\nFigure 15. GANomaly results on CIFAR10. All tests are here performed through a bce metric and using the area under the precision-recall\ncurve (AURPC). Training is done with a bce loss and a fm loss.\n\n\f","page0thumbnail":"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","metadata":{"pdfcpu":{"header":{"creation":"2024-08-20 16:25:49 CEST","version":"pdfcpu v0.8.0 dev"},"infos":[{"author":"Federico Di Mattia, Paolo Galeone, Michele De Simoni, Emanuele Ghelfi","creationDate":"D:20190628004906+00'00'","creator":"LaTeX with hyperref package","keywords":["Machine Learning","ICML","Deep Learning","Anomaly Detection","GANs"],"modificationDate":"D:20190628004906+00'00'","names":true,"pageCount":16,"pageMode":"UseNone","pageSizes":[{"height":792,"width":612}],"producer":"pdfTeX-1.40.17","properties":{"PTEX.Fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"},"source":"/tmp/blobproc-pdf-1997307425.pdf","title":"A Survey on GANs for Anomaly Detection","unit":"points","version":"1.5"}]},"pdfinfo":{"title":"A Survey on GANs for Anomaly Detection","keywords":"Machine Learning, ICML, Deep Learning, Anomaly Detection, GANs","author":"Federico Di Mattia, Paolo Galeone, Michele De Simoni, Emanuele Ghelfi","creator":"LaTeX with hyperref package","producer":"pdfTeX-1.40.17","creation_date":"Fri Jun 28 02:49:06 2019 CEST","mod_date":"Fri Jun 28 02:49:06 2019 CEST","custom_metadata":true,"form":"none","pages":16,"page_size":"612 x 792 pts (letter)","filesize":1215637,"pdf_version":"1.5"}},"pdfextra":{"page0height":792,"page0width":612,"page_count":16,"pdf_version":"1.5"},"weblinks":["http://arXiv","http://arxiv","http://arxiv.org/abs/1607.07539","http://arxiv.org/abs/1805.06725","http://dblp.uni-trier.de/db/journals/","http://kdd.ics.uci.edu/","http://papers.nips.cc/paper/","http://yann.lecun.com/exdb/","https://www.tensorflow.org/"]}
